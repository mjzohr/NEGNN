{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TkB5ia5ozObK"
      },
      "outputs": [],
      "source": [
        "seed = 1234"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MKenNAnFFHd"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YeGAzOLQXgP0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dgl-cu111 dglgo -f https://data.dgl.ai/wheels/repo.html\n",
        "# !pip install dgl dglgo -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "%matplotlib inline\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import networkx as nx\n",
        "\n",
        "import argparse\n",
        "import math \n",
        "import time\n",
        "import random\n",
        "import itertools\n",
        "import heapq\n",
        "from tqdm import tqdm\n",
        "from dgl.data import register_data_args\n",
        "\n",
        "seed = seed\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "dgl.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZk4YbP6SMLm"
      },
      "source": [
        "### Neighborhood extention functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ymEWs0MVIUP6"
      },
      "outputs": [],
      "source": [
        "class ExtendNeighborhood():\n",
        "\n",
        "    def __init__(self, graph, graph_undirected, alpha, beta, gamma, explore, extend_metric, ebunch=None):\n",
        "        self.graph = graph\n",
        "        self.graph_undirected = graph_undirected\n",
        "        self.extend_metric = extend_metric\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.explore = explore\n",
        "        self.ebunch = ebunch\n",
        "\n",
        "    def calc_score_aa(self, u, v):         \n",
        "        common_neighs = list(nx.common_neighbors(self.graph_undirected, u, v))\n",
        "        if len(common_neighs) > 0: \n",
        "            # scores = map(lambda x: 1 / math.log(self.graph_undirected.degree(x)), common_neighs)\n",
        "            scores = []\n",
        "            for x in common_neighs: \n",
        "                d = math.log(self.graph_undirected.degree(x))\n",
        "                if d > 0:\n",
        "                    scores.append(1/math.log(d))\n",
        "                else:\n",
        "                  continue\n",
        "            return sum(scores)\n",
        "\n",
        "    def add_edges_centrality_based(self):\n",
        "        # these ones return a Dictionary of nodes with centrality as the value.\n",
        "        if self.extend_metric == 'degree':\n",
        "            centrality = nx.degree_centrality(self.graph_undirected)\n",
        "        elif self.extend_metric == 'eigenvector':\n",
        "            di_graph = nx.DiGraph(self.graph_undirected)\n",
        "            centrality = nx.eigenvector_centrality(di_graph)\n",
        "\n",
        "        important_nodes = heapq.nlargest(self.alpha, centrality, key=centrality.get)\n",
        "        \n",
        "        # TODO, should the selected nodes change for each imp_node?\n",
        "        new_dataset = self.graph\n",
        "        for item in important_nodes:\n",
        "            tgt_nodes = set(self.graph_undirected) - set(self.graph_undirected[item])\n",
        "            selected_nodes = torch.tensor(np.random.choice(list(tgt_nodes), self.beta))\n",
        "            important_node = torch.ones(len(selected_nodes), dtype=int)*torch.tensor(item)\n",
        "            new_dataset = dgl.add_edges(new_dataset, selected_nodes, important_node)\n",
        "            new_dataset = dgl.add_edges(new_dataset, important_node, selected_nodes) \n",
        "\n",
        "        # new_dataset = self.graph\n",
        "        # for item in important_nodes:\n",
        "        #     tgt_nodes = set(self.graph_undirected) - set(self.graph_undirected[item])\n",
        "        #     important_node = torch.ones(len(tgt_nodes), dtype=int)*torch.tensor(item)\n",
        "        #     candidate_edges = list(zip(important_node, tgt_nodes))\n",
        "            \n",
        "        #     edges = []\n",
        "        #     scores = []\n",
        "        #     for edge in candidate_edges:\n",
        "        #         u, v = edge\n",
        "        #         try:\n",
        "        #             pred_val = sum(1 / math.log(self.graph_undirected.degree(w)) for w in nx.common_neighbors(self.graph_undirected, u, v))\n",
        "        #             # pred_val = calc_score(u, v)\n",
        "        #             if pred_val > 0:\n",
        "        #                 edges.append((u,v))\n",
        "        #                 scores.append(pred_val)\n",
        "        #         except:\n",
        "        #             continue\n",
        "        #     scores = np.array(scores)\n",
        "        #     sum_scores = scores.sum()\n",
        "        #     scores /= sum_scores\n",
        "\n",
        "        #     if len(edges) > self.beta:\n",
        "        #         rnd_indices = np.random.choice(len(edges), int(self.gamma), p=scores, replace=False)\n",
        "        #         edges = [edges[i] for i in rnd_indices]\n",
        "                \n",
        "        #     extra_random = self.beta - len(edges)\n",
        "        #     if extra_random > 0:\n",
        "        #         rnd_indices = np.random.choice(len(candidate_edges), int(extra_random), replace=False)\n",
        "        #         extra_edges = [candidate_edges[i] for i in rnd_indices]\n",
        "        #         edges.extend(extra_edges)\n",
        "\n",
        "        #     selected_edges = torch.tensor(edges)\n",
        "        #     new_edges_1 = selected_edges[:,0]\n",
        "        #     new_edges_2 = selected_edges[:,1]\n",
        "\n",
        "        #     new_dataset = dgl.add_edges(new_dataset, new_edges_1, new_edges_2)\n",
        "        #     new_dataset = dgl.add_edges(new_dataset, new_edges_2, new_edges_1)\n",
        "\n",
        "        return new_dataset\n",
        "\n",
        "\n",
        "    def non_edges_important_nodes(self):\n",
        "        centrality = nx.degree_centrality(self.graph_undirected)\n",
        "        scores = list( np.array(list(centrality.values())) / np.sum(list(centrality.values())) )\n",
        "        selected_src_nodes = np.random.choice(list(centrality.keys()), self.alpha, p=scores, replace=False)\n",
        "        # selected_src_nodes = heapq.nlargest(self.alpha, centrality, key=centrality.get)\n",
        "        edges_list = []\n",
        "        # for u in tqdm(selected_src_nodes):\n",
        "        for u in selected_src_nodes:\n",
        "            tgt_nodes = set(self.graph_undirected) - set(self.graph_undirected[u])\n",
        "            selected_tgt_nodes = np.random.choice(list(tgt_nodes), int(self.explore*len(tgt_nodes)), replace=False)\n",
        "            edges = list(zip(np.ones(len(selected_tgt_nodes), dtype=int)*u, selected_tgt_nodes))\n",
        "            edges_list.extend(edges)\n",
        "        return edges_list\n",
        "\n",
        "    def adamic_adar_index(self, ebunch):\n",
        "        def calc_score(u, v):         \n",
        "            common_neighs = list(nx.common_neighbors(self.graph_undirected, u, v))\n",
        "            if len(common_neighs) > 0: \n",
        "                # scores = map(lambda x: 1 / math.log(self.graph_undirected.degree(x)), common_neighs)\n",
        "                scores = []\n",
        "                for x in common_neighs: \n",
        "                    d = math.log(self.graph_undirected.degree(x))\n",
        "                    if d > 0:\n",
        "                        scores.append(1/math.log(d))\n",
        "                    else:\n",
        "                      continue\n",
        "                return sum(scores)\n",
        "                \n",
        "        edges = []\n",
        "        scores = []\n",
        "        # for edge in tqdm(ebunch):\n",
        "        for edge in ebunch:\n",
        "            u, v = edge\n",
        "            try:\n",
        "                pred_val = sum(1 / math.log(self.graph_undirected.degree(w)) for w in nx.common_neighbors(self.graph_undirected, u, v))\n",
        "                # pred_val = calc_score(u, v)\n",
        "\n",
        "                if pred_val > 0:\n",
        "                    edges.append((u,v))\n",
        "                    scores.append(pred_val)\n",
        "            except:\n",
        "                continue\n",
        "        scores = np.array(scores)\n",
        "        sum_scores = scores.sum()\n",
        "        scores /= sum_scores\n",
        "\n",
        "        # print(len(scores), len(edges))\n",
        "        return edges, scores\n",
        "\n",
        "    def resource_allocation_index(self, ebunch):\n",
        "        def calc_score(u, v):         \n",
        "            common_neighs = list(nx.common_neighbors(self.graph_undirected, u, v))\n",
        "            if len(common_neighs) > 0: \n",
        "                scores = []\n",
        "                for x in common_neighs: \n",
        "                    d = self.graph_undirected.degree(x)\n",
        "                    if d > 0:\n",
        "                        scores.append(d)\n",
        "                    else:\n",
        "                      continue\n",
        "                return sum(scores)\n",
        "\n",
        "        edges = []\n",
        "        scores = []\n",
        "        # for edge in tqdm(ebunch):\n",
        "        for edge in ebunch:\n",
        "            u, v = edge\n",
        "            try:\n",
        "              pred_val = sum(1 / self.graph_undirected.degree(w) for w in nx.common_neighbors(self.graph_undirected, u, v))\n",
        "              # pred_val = calc_score(u, v)\n",
        "              if pred_val > 0:\n",
        "                  edges.append((u,v))\n",
        "                  scores.append(pred_val)\n",
        "            except:\n",
        "              continue\n",
        "        scores = np.array(scores, dtype='float64')\n",
        "        sum_scores = scores.sum()\n",
        "        scores /= sum_scores\n",
        "        return edges, scores\n",
        "\n",
        "    def jaccard_coefficient_index(self, ebunch):\n",
        "        edges = []\n",
        "        scores = []\n",
        "        # for edge in tqdm(ebunch):\n",
        "        for edge in ebunch:\n",
        "            u, v = edge\n",
        "            try:\n",
        "              cnbors = list(nx.common_neighbors(self.graph_undirected, u, v))\n",
        "              union_size = len(set(self.graph_undirected[u]) | set(self.graph_undirected[v]))\n",
        "              if union_size == 0:\n",
        "                  pred_val = 0\n",
        "              else:\n",
        "                  pred_val = len(cnbors) / union_size\n",
        "              \n",
        "              if pred_val > 0:\n",
        "                  edges.append((u,v))\n",
        "                  scores.append(pred_val)\n",
        "            except:\n",
        "              continue\n",
        "        scores = np.array(scores)\n",
        "        sum_scores = scores.sum()\n",
        "        scores /= sum_scores\n",
        "        return edges, scores\n",
        "\n",
        "\n",
        "    def add_edges_similarity_based(self):\n",
        "        if self.alpha == self.graph_undirected.number_of_nodes() and self.explore == 1:\n",
        "            ebunch = nx.non_edges(self.graph_undirected)\n",
        "        else:\n",
        "            ebunch = self.non_edges_important_nodes()\n",
        "\n",
        "        # ss = nx.adamic_adar_index(self.graph_undirected, ebunch)\n",
        "        # print(list(ss))\n",
        "        if self.extend_metric == 'resource_alloc':\n",
        "            edges, scores = self.resource_allocation_index(ebunch)\n",
        "        if self.extend_metric == 'jaccard':\n",
        "            edges, scores = self.jaccard_coefficient_index(ebunch)\n",
        "        if self.extend_metric == 'adamic_adar':\n",
        "            edges, scores = self.adamic_adar_index(ebunch)\n",
        "\n",
        "        # print()\n",
        "        # print(len(edges), len(scores))\n",
        "\n",
        "        if len(edges) > self.gamma:\n",
        "            rnd_indices = np.random.choice(len(edges), int(self.gamma), p=scores, replace=False)\n",
        "            edges = [edges[i] for i in rnd_indices]\n",
        "            \n",
        "            \n",
        "        extra_random = int(self.gamma - len(edges))\n",
        "        if extra_random > 0:\n",
        "            rnd_indices = np.random.choice(len(ebunch), int(extra_random), replace=False)\n",
        "            extra_edges = [ebunch[i] for i in rnd_indices]\n",
        "            edges.extend(extra_edges)\n",
        "\n",
        "        selected_edges = torch.tensor(edges)\n",
        "        new_edges_1 = selected_edges[:,0]\n",
        "        new_edges_2 = selected_edges[:,1]\n",
        "\n",
        "        new_dataset = dgl.add_edges(self.graph, new_edges_1, new_edges_2)\n",
        "        new_dataset = dgl.add_edges(new_dataset, new_edges_2, new_edges_1)\n",
        "\n",
        "        return new_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7yJ8zKggkwrs"
      },
      "outputs": [],
      "source": [
        "# extend_neighborhood = ExtendNeighborhood(g, graph_undirected, centrality_metric='degree', alpha=alpha, beta=beta, gamma=gamma, explore=explore, extend_metric=extend_metric)\n",
        "# if extend_metric == 'adamic_adar':\n",
        "#     print(\"---Adamic Adar index---\")\n",
        "#     extended_graph = extend_neighborhood.add_edges()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzhnNd60H7tl"
      },
      "source": [
        "### Load Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zU4X7Hn8R4pS"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R2Q79J0a8lKE"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_name, verbose=False):\n",
        "    if dataset_name == \"cora\": \n",
        "        dataset = dgl.data.CoraGraphDataset(verbose=False)\n",
        "    elif dataset_name == \"citeseer\":\n",
        "        dataset = dgl.data.CiteseerGraphDataset(verbose=False)\n",
        "    elif dataset_name == \"pubmed\":\n",
        "        dataset = dgl.data.PubmedGraphDataset(verbose=False)\n",
        "    elif dataset_name == \"amazonphoto\":\n",
        "        dataset = dgl.data.AmazonCoBuyPhotoDataset(verbose=False)\n",
        "    elif dataset_name == \"amazoncomputer\":\n",
        "        dataset = dgl.data.AmazonCoBuyComputerDataset(verbose=False)\n",
        "    else: \n",
        "      raise(\"Not defined!\")\n",
        "\n",
        "    graph_nx = dgl.to_networkx(dataset[0])\n",
        "    graph_undirected = nx.Graph(graph_nx)\n",
        "    g = dataset[0]\n",
        "\n",
        "    if (dataset_name == 'amazonphoto') or (dataset_name == 'amazoncomputer'): \n",
        "        length = g.num_nodes()\n",
        "        train_num = int(np.floor(0.7*length))\n",
        "        valid_num = int(np.floor(0.1*length))\n",
        "        test_num = int(np.ceil(0.2*length))\n",
        "        a = (np.zeros(train_num))\n",
        "        a_1 = (np.ones(valid_num))\n",
        "        a_2 = (np.ones(test_num)*2)\n",
        "\n",
        "        mask = np.hstack((a, a_1))\n",
        "        mask = np.hstack((mask, a_2))\n",
        "\n",
        "        np.random.shuffle(mask)\n",
        "\n",
        "        train_mask = torch.tensor(np.where(mask==0,True,False))\n",
        "        val_mask = torch.tensor( np.where(mask==1,True,False))\n",
        "        test_mask = torch.tensor(np.where(mask==2,True,False))\n",
        "\n",
        "        features = g.ndata['feat']\n",
        "        labels = g.ndata['label']\n",
        "        num_feats = features.shape[1]\n",
        "        n_classes = dataset.num_classes\n",
        "        n_edges = g.num_edges()\n",
        "\n",
        "    else:\n",
        "        features = g.ndata['feat']\n",
        "        labels = g.ndata['label']\n",
        "        train_mask = g.ndata['train_mask']\n",
        "        val_mask = g.ndata['val_mask']\n",
        "        test_mask = g.ndata['test_mask']\n",
        "        num_feats = features.shape[1]\n",
        "        n_classes = dataset.num_classes \n",
        "        n_edges = g.number_of_edges()\n",
        "\n",
        "    # if verbose:\n",
        "    #     print(\"\"\"----Data statistics------'\n",
        "    #       #Edges %d\n",
        "    #       #Classes %d\n",
        "    #       #Train samples %d\n",
        "    #       #Val samples %d\n",
        "    #       #Test samples %d\"\"\" %\n",
        "    #           (n_edges, n_classes,\n",
        "    #             train_mask.int().sum().item(),\n",
        "    #             val_mask.int().sum().item(),\n",
        "    #             test_mask.int().sum().item()))\n",
        "        \n",
        "    return g, graph_nx, graph_undirected, features, labels, train_mask, val_mask, test_mask, num_feats, n_classes, n_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qDlU7K7rgU9K"
      },
      "outputs": [],
      "source": [
        "# print(dataset_name)\n",
        "# g, graph_nx, graph_undirected, features, labels, train_mask, val_mask, test_mask, num_feats, n_classes, n_edges = load_dataset(dataset_name, verbose=True)\n",
        "# features_back, labels_back = features, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoLS4SrUWdBY"
      },
      "source": [
        "### GNN models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Oj-NALRvzoU-"
      },
      "outputs": [],
      "source": [
        "from dgl.nn import SAGEConv\n",
        "from dgl.nn import GraphConv\n",
        "from dgl.nn import GATConv\n",
        "from dgl.nn import GATv2Conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_YwzxVKguS8m"
      },
      "outputs": [],
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 num_classes,\n",
        "                 heads,\n",
        "                 activation,\n",
        "                 feat_drop,\n",
        "                 attn_drop,\n",
        "                 negative_slope,\n",
        "                 residual):\n",
        "        super(GAT, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.gat_layers.append(GATConv(\n",
        "            in_dim, num_hidden, heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for l in range(1, num_layers):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.gat_layers.append(GATConv(\n",
        "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
        "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "        # output projection\n",
        "        self.gat_layers.append(GATConv(\n",
        "            num_hidden * heads[-2], num_classes, heads[-1],\n",
        "            feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.gat_layers[-1](g, h).mean(1)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "28QXsmO51baW"
      },
      "outputs": [],
      "source": [
        "class GATv2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 num_classes,\n",
        "                 heads,\n",
        "                 activation,\n",
        "                 feat_drop,\n",
        "                 attn_drop,\n",
        "                 negative_slope,\n",
        "                 residual):\n",
        "        super(GATv2, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.gat_layers.append(GATv2Conv(\n",
        "            in_dim, num_hidden, heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for l in range(1, num_layers):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.gat_layers.append(GATv2Conv(\n",
        "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
        "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "        # output projection\n",
        "        self.gat_layers.append(GATv2Conv(\n",
        "            num_hidden * heads[-2], num_classes, heads[-1],\n",
        "            feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.gat_layers[-1](g, h).mean(1)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7YyxBjA0WoM1"
      },
      "outputs": [],
      "source": [
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
        "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
        "        # self.conv1 = SAGEConv(in_feats, h_feats, 'gcn')\n",
        "        # self.conv2 = SAGEConv(h_feats, h_feats, 'gcn')\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, in_feats, h_feats):\n",
        "      super(GCN, self).__init__()\n",
        "      self.conv1 = GraphConv(in_feats, h_feats)\n",
        "      self.conv2 = GraphConv(h_feats, h_feats)\n",
        "  \n",
        "  def forward(self, g, in_feat):\n",
        "      h = self.conv1(g, in_feat)\n",
        "      h = F.relu(h)\n",
        "      h = self.conv2(g, h)\n",
        "      return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4g8KklyhYa6"
      },
      "source": [
        "### Train & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YCS9qQvYBRul"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def step(self, acc, model):\n",
        "        score = acc\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            # print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        torch.save(model.state_dict(), 'es_checkpoint.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Fdb8ARXAAO8T"
      },
      "outputs": [],
      "source": [
        "def accuracy(logits, labels):\n",
        "    _, indices = torch.max(logits, dim=1)\n",
        "    correct = torch.sum(indices == labels)\n",
        "    return correct.item() * 1.0 / len(labels)\n",
        "\n",
        "def evaluate(model, features, labels, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(g, features)\n",
        "        logits = logits[mask]\n",
        "        labels = labels[mask]\n",
        "        return accuracy(logits, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WBBlJBREJKcw"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "lr = 0.005\n",
        "weight_decay = 5e-4\n",
        "epochs = 200\n",
        "fastmode = False\n",
        "early_stop = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTlW0jhgQUpc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H54z2YmJSVhw"
      },
      "source": [
        "### Run function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uJxx3hKJSYNL"
      },
      "outputs": [],
      "source": [
        "def run(seed, dataset_name, model_name, alpha, beta, gamma, explore, extend_metric):\n",
        "\n",
        "    def accuracy(logits, labels):\n",
        "        _, indices = torch.max(logits, dim=1)\n",
        "        correct = torch.sum(indices == labels)\n",
        "        return correct.item() * 1.0 / len(labels)\n",
        "\n",
        "    def evaluate(model, features, labels, mask):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(g, features)\n",
        "            logits = logits[mask]\n",
        "            labels = labels[mask]\n",
        "            return accuracy(logits, labels)\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    dgl.random.seed(seed)\n",
        "\n",
        "    g, graph_nx, graph_undirected, features, labels, train_mask, val_mask, test_mask, num_feats, n_classes, n_edges = load_dataset(dataset_name, verbose=True)\n",
        "\n",
        "    if model_name == 'sage':\n",
        "        # print(\"SAGE model is used\")\n",
        "        model = GraphSAGE(g.ndata['feat'].shape[1], 16)\n",
        "    elif model_name == 'gcn':\n",
        "        # print(\"GCN model is used\")\n",
        "        model = GCN(g.ndata['feat'].shape[1], 16)\n",
        "    elif model_name == 'gat':\n",
        "        # print(\"GAT model is used\")\n",
        "        num_heads, num_layers, num_out_heads = 8, 2, 1\n",
        "        num_hidden = n_classes = 16\n",
        "        in_drop, attn_drop, negative_slope, residual = 0.6, 0.6, 0.2, False\n",
        "        heads = ([num_heads] * num_layers) + [num_out_heads]\n",
        "        model = GAT(num_layers, g.ndata['feat'].shape[1], num_hidden, n_classes, heads, F.elu, in_drop, attn_drop, negative_slope, residual)\n",
        "    elif model_name == 'gatv2':\n",
        "        # print(\"GATv2 model is used\")\n",
        "        num_heads, num_layers, num_out_heads = 8, 2, 1\n",
        "        num_hidden = n_classes = 16\n",
        "        in_drop, attn_drop, negative_slope, residual = 0.6, 0.6, 0.2, False\n",
        "        heads = ([num_heads] * num_layers) + [num_out_heads]\n",
        "        model = GATv2(num_layers, g.ndata['feat'].shape[1], num_hidden, n_classes, heads, F.elu, in_drop, attn_drop, negative_slope, residual)\n",
        "    else:\n",
        "        raise(\"Not a model!\")\n",
        "\n",
        "\n",
        "    if extend_metric != 'None':\n",
        "        t1 = time.time()\n",
        "        num_edge_before_extention = g.number_of_edges()\n",
        "        # print(f'Total number of edges before extension: {num_edge_before_extention}')\n",
        "        extend_neighborhood = ExtendNeighborhood(g, graph_undirected, alpha=alpha, beta=beta, gamma=gamma, explore=explore, extend_metric=extend_metric)\n",
        "        # print(extend_metric)\n",
        "        if extend_metric == 'degree' or extend_metric == 'eigenvector':\n",
        "            extended_graph = extend_neighborhood.add_edges_centrality_based()\n",
        "        else: \n",
        "            extended_graph = extend_neighborhood.add_edges_similarity_based()\n",
        "        g = extended_graph\n",
        "        t_total = time.time() - t1\n",
        "        num_edge_after_extention = g.number_of_edges()\n",
        "        # print()\n",
        "        # print(f'Total number of edges after extension: {num_edge_after_extention}')\n",
        "        # print(f'Total number of added edges: {num_edge_after_extention - num_edge_before_extention}')\n",
        "        # print(f\"*** Total construction time in seconds: {t_total:.2f} ***\")\n",
        "    else:\n",
        "        print(\"Neighborhood is not extended!\")\n",
        "\n",
        "    if add_self_loop == True:\n",
        "        # print(f\"Total edges before adding self-loop {g.number_of_edges()}\")\n",
        "        g = g.remove_self_loop().add_self_loop()\n",
        "        # print(f\"Total edges after adding self-loop {g.number_of_edges()}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    g, labels, features, train_mask, val_mask, test_mask = map(lambda x: x.to(device), (g, labels, features, train_mask, val_mask, test_mask))\n",
        "    \n",
        "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # if early_stop:\n",
        "    #     stopper = EarlyStopping(patience=100)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        # forward\n",
        "        logits = model(g, features)\n",
        "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc = accuracy(logits[train_mask], labels[train_mask])\n",
        "\n",
        "        if fastmode:\n",
        "            val_acc = accuracy(logits[val_mask], labels[val_mask])\n",
        "        else:\n",
        "            val_acc = evaluate(model, features, labels, val_mask)\n",
        "            # if early_stop:\n",
        "            #     if stopper.step(val_acc, model):\n",
        "            #         break\n",
        "        \n",
        "        # if epoch % 10 == 0:\n",
        "        #     print(\"Epoch {:05d} | Loss {:.4f} | TrainAcc {:.4f} |\"\n",
        "        #         \" ValAcc {:.4f}\".\n",
        "        #         format(epoch, loss.item(), train_acc,\n",
        "        #                 val_acc))\n",
        "\n",
        "    # val_acc = evaluate(model, features, labels, val_mask)\n",
        "    acc = evaluate(model, features, labels, test_mask)\n",
        "\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2qB9kQyHlYr"
      },
      "source": [
        "## Run Experiments: Centrality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIO5zaUk6rXb"
      },
      "source": [
        "### Experiments One"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEG40SjPOLRG"
      },
      "outputs": [],
      "source": [
        "# seed = 1000\n",
        "# random.seed(seed)\n",
        "# np.random.seed(seed)\n",
        "# torch.manual_seed(seed)\n",
        "# torch.cuda.manual_seed(seed)\n",
        "# torch.cuda.manual_seed_all(seed)\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "# dgl.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK2MsBFjDmHa"
      },
      "outputs": [],
      "source": [
        "# dataset_name = 'cora'\n",
        "# dataset_name = 'citeseer'\n",
        "dataset_name = 'pubmed'\n",
        "# dataset_name = 'amazonphoto'\n",
        "# dataset_name = 'amazoncomputer'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNfw76vsWeJR"
      },
      "outputs": [],
      "source": [
        "seed = 1000\n",
        "# seed = 4\n",
        "add_self_loop = True\n",
        "\n",
        "# dummies\n",
        "gamma = explore = alpha = beta = gamma = 1\n",
        "\n",
        "# model_names = ['gcn', 'sage', 'gat', 'gatv2']\n",
        "model_name = 'gatv2'\n",
        "\n",
        "# extend_metrics = [\"degree\", \"eigenvector\"]\n",
        "extend_metrics = [\"degree\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHSifJID_-Ul"
      },
      "outputs": [],
      "source": [
        "for i in range(0, 200):\n",
        "    original_acc = run(seed=i, dataset_name=dataset_name, extend_metric='None', model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore)\n",
        "    print(i, original_acc)\n",
        "    # if original_acc == 0.801:\n",
        "        # print(i)\n",
        "        # break  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plqQ2QxpOeuc"
      },
      "source": [
        "Test one!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga93tPAsVUmD"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'cora'\n",
        "model_name = 'gcn'\n",
        "seed = 3\n",
        "\n",
        "original_acc = run(seed=seed, dataset_name=dataset_name, extend_metric='None', model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore)\n",
        "print(original_acc) \n",
        "\n",
        "betas = [3]\n",
        "# betas = [1]\n",
        "alphas = [388]\n",
        "\n",
        "for beta in betas: \n",
        "  for alpha in alphas:\n",
        "    test_run = run(seed=seed, dataset_name=dataset_name, extend_metric='degree', \\\n",
        "                  model_name=model_name, alpha=alpha, beta=beta, gamma=1, explore=1)\n",
        "    print(\"alpha:\", alpha, \"  beta:\", beta, \" | res:\", test_run) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfRQEPPeO9Xz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XymoOu7Vr4x4"
      },
      "source": [
        "### Expreiments full "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNPvX_4jjfUK"
      },
      "source": [
        "#### Test Settings and Run function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_lNJxPfw8QT"
      },
      "outputs": [],
      "source": [
        "def test_settings(name):\n",
        "    if name == 'cora':\n",
        "        num_nodes = 2708\n",
        "        test_list_alpha_centrality = []\n",
        "        test_list_alpha_similarity = []\n",
        "        test_list_beta = []\n",
        "\n",
        "        percents = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "        percents = [int(num*num_nodes) for num in percents]\n",
        "        # print(percents)\n",
        "\n",
        "        for i in range(1, num_nodes):\n",
        "            if i < 100:\n",
        "                if i % 20 == 0:\n",
        "                    test_list_alpha_centrality.append(i)\n",
        "            if 100 <= i < 1000:\n",
        "                if i % 80 == 0:\n",
        "                    test_list_alpha_centrality.append(i)\n",
        "            if i < 6:\n",
        "                test_list_beta.append(i)\n",
        "            if 10 <= i < 100:\n",
        "                if i % 20 == 0: \n",
        "                    test_list_beta.append(i)\n",
        "            if i in percents:\n",
        "                test_list_alpha_similarity.append(i)\n",
        "    elif name == 'citeseer':\n",
        "        num_nodes = 3327\n",
        "        test_list_alpha_centrality = []\n",
        "        test_list_alpha_similarity = []\n",
        "        test_list_beta = []\n",
        "\n",
        "        percents = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "        percents = [int(num*num_nodes) for num in percents]\n",
        "        # print(percents)\n",
        "\n",
        "        for i in range(1, num_nodes):\n",
        "            if i < 100:\n",
        "                if i % 20 == 0:\n",
        "                    test_list_alpha_centrality.append(i)\n",
        "            if 100 <= i < 1200:\n",
        "                if i % 100 == 0:\n",
        "                    test_list_alpha_centrality.append(i)\n",
        "            if i < 5:\n",
        "                test_list_beta.append(i)\n",
        "            if 10 <= i < 100:\n",
        "                if i % 20 == 0: \n",
        "                    test_list_beta.append(i)\n",
        "            if i in percents:\n",
        "                test_list_alpha_similarity.append(i)\n",
        "    elif name == 'pubmed':\n",
        "        num_nodes = 19717\n",
        "        test_list_alpha_centrality = []\n",
        "        test_list_alpha_similarity = []\n",
        "        test_list_beta = []\n",
        "\n",
        "        percents = [0.5, 0.6, 0.7, 0.8]\n",
        "        percents = [int(num*num_nodes) for num in percents]\n",
        "        # print(percents)\n",
        "\n",
        "        for i in range(1, num_nodes):\n",
        "            if i < 300:\n",
        "                if i % 50 == 0:\n",
        "                    test_list_alpha_centrality.append(i)\n",
        "            if 300 < i < 10000:\n",
        "                if i % 700 == 0:\n",
        "                    test_list_alpha_centrality.append(i)\n",
        "            if i < 5:\n",
        "                test_list_beta.append(i)\n",
        "            if i in percents:\n",
        "                test_list_alpha_similarity.append(i)\n",
        "    elif name == 'amazonphoto':\n",
        "        num_nodes = 7650\n",
        "        test_list_alpha_centrality = []\n",
        "        test_list_alpha_similarity = []\n",
        "        test_list_beta = []\n",
        "\n",
        "        percents = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "        percents = [int(num*num_nodes) for num in percents]\n",
        "        # print(percents)\n",
        "\n",
        "        for i in range(1, num_nodes):\n",
        "            if i < 300:\n",
        "                if i % 50 == 0:\n",
        "                    test_list_alpha_centrality.append(i)\n",
        "            if 500 < i < 10000:\n",
        "                if i % 800 == 0:\n",
        "                      test_list_alpha_centrality.append(i)\n",
        "            if i < 5:\n",
        "                test_list_beta.append(i)\n",
        "            if i in percents:\n",
        "                test_list_alpha_similarity.append(i)\n",
        "    elif name == 'amazoncomputer':\n",
        "        num_nodes = 13752\n",
        "        test_list_alpha_centrality = []\n",
        "        test_list_alpha_similarity = []\n",
        "        test_list_beta = []\n",
        "\n",
        "        percents = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "        percents = [int(num*num_nodes) for num in percents]\n",
        "        # print(percents)\n",
        "\n",
        "        for i in range(1, num_nodes):\n",
        "            if i < 500:\n",
        "                if i % 50 == 0:\n",
        "                    test_list_alpha_centrality.append(i)\n",
        "            if 500 < i < 8000:\n",
        "                if i % 1000 == 0:\n",
        "                    test_list_alpha_centrality.append(i)\n",
        "            if i < 5:\n",
        "                test_list_beta.append(i)\n",
        "            if i in percents:\n",
        "                test_list_alpha_similarity.append(i)\n",
        "    else: \n",
        "          raise(\"Not defined!\")\n",
        "    \n",
        "    return test_list_alpha_centrality, test_list_alpha_similarity, test_list_beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ta6vNWeWVM"
      },
      "outputs": [],
      "source": [
        "add_self_loop = True\n",
        "def run_experiments(seed, dataset_name, model_name, extend_metrics):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    dgl.random.seed(seed)\n",
        "\n",
        "    gamma = explore = alpha = beta = gamma = 1\n",
        "    results_file = dataset_name + '_results'\n",
        "\n",
        "    original_acc = run(seed=seed, dataset_name=dataset_name, extend_metric='None', model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore)\n",
        "    results_file = f\"{dataset_name}_{model_name}.txt\"\n",
        "    with open(results_file, 'w') as f:\n",
        "        f.write(\"Original, model:{}, accuracy:{}, seed:{}\".format(model_name, original_acc, seed))\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"{},{},{},{},{}\".format(\"accuracy\", \"alpha\", \"beta\", \"model_name\", \"extend_metric\"))\n",
        "        f.write(\"\\n--------------------------------------------\\n\")\n",
        "\n",
        "    test_list_alpha_centrality, test_list_alpha_similarity, test_list_beta = test_settings(dataset_name)\n",
        "\n",
        "    for extend_metric in extend_metrics:\n",
        "      for alpha in tqdm(test_list_alpha_centrality):\n",
        "        for beta in test_list_beta:\n",
        "\n",
        "              acc = run(seed=seed, dataset_name=dataset_name, model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore, extend_metric=extend_metric)\n",
        "\n",
        "              if acc >= original_acc:\n",
        "                  with open(results_file, 'a') as f:\n",
        "                      f.write(\"{},{},{},{},{}\".format(acc, alpha, beta, model_name, extend_metric))\n",
        "                      f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nuXpJonjlIJ"
      },
      "source": [
        "#### Run "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUBF-pf3kp9g"
      },
      "outputs": [],
      "source": [
        "# dataset_name: cora, citeseer, pubmed, amazonphoto, amazoncomputer\n",
        "# model_name: gat, gatv2, gcn, sage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-hBxGv3jo5Y"
      },
      "outputs": [],
      "source": [
        "# run_experiments(seed=31, dataset_name='cora', model_name='gat', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=31, dataset_name='cora', model_name='gatv2', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=31, dataset_name='cora', model_name='gcn', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=31, dataset_name='cora', model_name='sage', extend_metrics = [\"degree\", \"eigenvector\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fue3tCTY0Nsx"
      },
      "outputs": [],
      "source": [
        "# run_experiments(seed=138, dataset_name='citeseer', model_name='gat', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=138, dataset_name='citeseer', model_name='gatv2', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=10, dataset_name='citeseer', model_name='gcn', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=10, dataset_name='citeseer', model_name='sage', extend_metrics = [\"degree\", \"eigenvector\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGy6Nxyb4rBR"
      },
      "outputs": [],
      "source": [
        "# run_experiments(seed=10, dataset_name='pubmed', model_name='gat', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=10, dataset_name='pubmed', model_name='gatv2', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=123, dataset_name='pubmed', model_name='gcn', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=123, dataset_name='pubmed', model_name='sage', extend_metrics = [\"degree\", \"eigenvector\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IxZCkOmt0iH"
      },
      "outputs": [],
      "source": [
        "# run_experiments(seed=11, dataset_name='amazonphoto', model_name='gat', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=10, dataset_name='amazonphoto', model_name='gatv2', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=123, dataset_name='amazonphoto', model_name='gcn', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=123, dataset_name='amazonphoto', model_name='sage', extend_metrics = [\"degree\", \"eigenvector\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KZGu-ysuQW4"
      },
      "outputs": [],
      "source": [
        "# run_experiments(seed=11, dataset_name='amazoncomputer', model_name='gat', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=10, dataset_name='amazonphoto', model_name='gatv2', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=123, dataset_name='amazoncomputer', model_name='gcn', extend_metrics = [\"degree\", \"eigenvector\"])\n",
        "# run_experiments(seed=30, dataset_name='amazoncomputer', model_name='sage', extend_metrics = [\"degree\", \"eigenvector\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qob-4GKor3bN"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download('/content/pubmed_gat.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJv4BZ3gNa6S"
      },
      "source": [
        "## Run Experiments: Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ligQibMc5v2Q"
      },
      "source": [
        "### Experiment One"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK0RlIuPrBeG"
      },
      "outputs": [],
      "source": [
        "# beta = 1\n",
        "# add_self_loop = True\n",
        "# dataset_name = 'amazonphoto'\n",
        "# seed = 1000\n",
        "# model_name = 'gatv2'\n",
        "\n",
        "# acc = run(seed=seed, dataset_name=dataset_name, model_name=model_name, alpha=4125, \\\n",
        "#           beta=beta, gamma=5600, explore=0.01, extend_metric='adamic_adar')\n",
        "\n",
        "# acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n8yqFNoUtUY"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'cora'\n",
        "num_nodes = 2708\n",
        "\n",
        "# dataset_name = 'citeseer'\n",
        "# num_nodes = 3327\n",
        "\n",
        "# dataset_name = 'pubmed'\n",
        "# num_nodes = 19717\n",
        "\n",
        "# dataset_name = 'amazoncomputer'\n",
        "# num_nodes = 13752\n",
        "\n",
        "# dataset_name = 'amazonphoto'\n",
        "# num_nodes = 7650"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbbV_xLR9Kgx"
      },
      "outputs": [],
      "source": [
        "num_nodes * 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw4tgP9ROBxS"
      },
      "outputs": [],
      "source": [
        "model_name = 'gatv2'\n",
        "add_self_loop = True\n",
        "# extend_metric = 'adamic_adar'\n",
        "# extend_metric = \"resource_alloc\"\n",
        "# extend_metric = \"jaccard\"\n",
        "extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"]\n",
        "\n",
        "seed = 1234\n",
        "# seed = 1000\n",
        "beta = 1    # dummies\n",
        "\n",
        "# alphas = [int(num_nodes*percent*0.1) for percent in range(1, 9, 1)]\n",
        "# gamma = 3200\n",
        "# explore = 0.01\n",
        "\n",
        "# for alpha in alphas:\n",
        "#     for extend_metric in extend_metrics:\n",
        "#         test_run = run(seed=seed, dataset_name=dataset_name, extend_metric=extend_metric, \\\n",
        "#                         model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore)\n",
        "#         print(f\"{extend_metric},{alpha},{gamma},{explore}\", \" | res:\", test_run) \n",
        "#     print()\n",
        "\n",
        "# ==========================================================\n",
        "\n",
        "alpha = int(num_nodes*0.9)\n",
        "gammas = [i for i in range(300, 1200, 100)]\n",
        "explore = 0.1\n",
        "\n",
        "for gamma in gammas:\n",
        "    for extend_metric in extend_metrics:\n",
        "        test_run = run(seed=seed, dataset_name=dataset_name, extend_metric=extend_metric, \\\n",
        "                        model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore)\n",
        "        print(f\"{extend_metric},{alpha},{gamma},{explore}\", \" | res:\", test_run) \n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA2kIR205PCP"
      },
      "outputs": [],
      "source": [
        "alpha = int(num_nodes*0.2)\n",
        "gammas = [1700, 1750, 1800, 1850, 1900, 1950, 2000]\n",
        "explore = 0.1\n",
        "\n",
        "for gamma in gammas:\n",
        "    for extend_metric in extend_metrics:\n",
        "        test_run = run(seed=seed, dataset_name=dataset_name, extend_metric=extend_metric, \\\n",
        "                        model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore)\n",
        "        print(f\"{extend_metric},{alpha},{gamma},{explore}\", \" | res:\", test_run) \n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adQxeXk55cFM"
      },
      "outputs": [],
      "source": [
        "alpha = int(num_nodes*0.2)\n",
        "gammas = [int(i) for i in range(2500, 2800, 50)]\n",
        "explore = 0.01\n",
        "\n",
        "for gamma in gammas:\n",
        "    for extend_metric in extend_metrics:\n",
        "        test_run = run(seed=seed, dataset_name=dataset_name, extend_metric=extend_metric, \\\n",
        "                        model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore)\n",
        "        print(f\"{extend_metric},{alpha},{gamma},{explore}\", \" | res:\", test_run) \n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XMCKcmxxURO"
      },
      "source": [
        "### Experiments Full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwokTUzhs_KF"
      },
      "source": [
        "#### Test Settings and Run function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OWjToQ6Gs_KG"
      },
      "outputs": [],
      "source": [
        "def test_settings_similarity(name):\n",
        "    if name == 'cora':\n",
        "        num_nodes = 2708\n",
        "        test_list_gamma = []\n",
        "        test_list_explore = [0.1]\n",
        "        percents = [0.4, 0.6, 0.8, 1]\n",
        "        test_list_alpha_similarity = [int(num*num_nodes) for num in percents]\n",
        "        for i in range(num_nodes):\n",
        "            if i > 300: \n",
        "                if i % 200 == 0:\n",
        "                    test_list_gamma.append(i)\n",
        "\n",
        "    elif name == 'citeseer':\n",
        "        num_nodes = 3327\n",
        "        test_list_gamma = []\n",
        "        test_list_explore = [0.1]\n",
        "        percents = [0.4, 0.6, 0.8, 1]\n",
        "        test_list_alpha_similarity = [int(num*num_nodes) for num in percents]\n",
        "        for i in range(num_nodes):\n",
        "            if i > 300: \n",
        "                if i % 200 == 0:\n",
        "                    test_list_gamma.append(i)\n",
        "\n",
        "    elif name == 'pubmed':\n",
        "        num_nodes = 19717\n",
        "        test_list_gamma = []\n",
        "        test_list_explore = [0.1]\n",
        "        percents = [0.2]\n",
        "        test_list_alpha_similarity = [int(num*num_nodes) for num in percents]\n",
        "        for i in range(num_nodes):\n",
        "            if 799 < i < 10000: \n",
        "                if i % 800 == 0:\n",
        "                    test_list_gamma.append(i)\n",
        "\n",
        "    elif name == 'amazonphoto':\n",
        "        num_nodes = 7650\n",
        "        test_list_gamma = []\n",
        "        test_list_explore = [0.004, 0.01]\n",
        "\n",
        "        percents = [0.6]\n",
        "        test_list_alpha_similarity = [int(num*num_nodes) for num in percents]\n",
        "        # print(percents)\n",
        "\n",
        "        for i in range(1, num_nodes):\n",
        "                if i % 800 == 0:\n",
        "                    test_list_gamma.append(i)\n",
        "    elif name == 'amazoncomputer':\n",
        "        num_nodes = 13752\n",
        "        test_list_gamma = []\n",
        "        test_list_explore = [0.01]\n",
        "        percents = [0.3]\n",
        "        test_list_alpha_similarity = [int(num*num_nodes) for num in percents]\n",
        "        for i in range(1, num_nodes):\n",
        "                if i % 800 == 0:\n",
        "                    test_list_gamma.append(i)\n",
        "\n",
        "    else: \n",
        "          raise(\"Not defined!\")\n",
        "    \n",
        "    return test_list_alpha_similarity, test_list_gamma, test_list_explore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XJlX3tJ4s_KH"
      },
      "outputs": [],
      "source": [
        "add_self_loop = True\n",
        "def run_experiments_similarity(seed, dataset_name, model_name, extend_metrics):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    dgl.random.seed(seed)\n",
        "\n",
        "    gamma = explore = alpha = beta = gamma = 1\n",
        "    results_file = dataset_name + '_results'\n",
        "\n",
        "    original_acc = run(seed=seed, dataset_name=dataset_name, extend_metric='None', model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore)\n",
        "    results_file = f\"{dataset_name}_{model_name}.txt\"\n",
        "    with open(results_file, 'w') as f:\n",
        "        f.write(\"Original, model:{}, accuracy:{}, seed:{}\".format(model_name, original_acc, seed))\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"{},{},{},{},{},{}\".format(\"accuracy\", \"alpha\", \"gamma\", \"explore\", \"model_name\", \"extend_metric\"))\n",
        "        f.write(\"\\n--------------------------------------------\\n\")\n",
        "\n",
        "    test_list_alpha_similarity, test_list_gamma, test_list_explore = test_settings_similarity(dataset_name)\n",
        "\n",
        "    for extend_metric in extend_metrics:\n",
        "      for explore in test_list_explore:\n",
        "        for alpha in test_list_alpha_similarity:\n",
        "          for gamma in tqdm(test_list_gamma):\n",
        "              acc = run(seed=seed, dataset_name=dataset_name, model_name=model_name, alpha=alpha, beta=beta, gamma=gamma, explore=explore, extend_metric=extend_metric)\n",
        "              if acc >= original_acc:\n",
        "                  with open(results_file, 'a') as f:\n",
        "                      f.write(\"{},{},{},{},{},{}\".format(acc, alpha, gamma, explore, model_name, extend_metric))\n",
        "                      f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a42AlsGkwXa-"
      },
      "source": [
        "#### Run "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_OEb-SMwXa_"
      },
      "outputs": [],
      "source": [
        "# dataset_name: cora, citeseer, pubmed, amazonphoto, amazoncomputer\n",
        "# model_name: gat, gatv2, gcn, sage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvmd173OwXa_"
      },
      "outputs": [],
      "source": [
        "# run_experiments_similarity(seed=31, dataset_name='cora', model_name='gat', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=31, dataset_name='cora', model_name='gatv2', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=31, dataset_name='cora', model_name='gcn', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=31, dataset_name='cora', model_name='sage', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6ZSIwUc3wXa_"
      },
      "outputs": [],
      "source": [
        "# run_experiments_similarity(seed=138, dataset_name='citeseer', model_name='gat', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=138, dataset_name='citeseer', model_name='gatv2', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=10, dataset_name='citeseer', model_name='gcn', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=10, dataset_name='citeseer', model_name='sage', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7_-8biKwXa_"
      },
      "outputs": [],
      "source": [
        "# run_experiments_similarity(seed=10, dataset_name='pubmed', model_name='gat', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=10, dataset_name='pubmed', model_name='gatv2', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=123, dataset_name='pubmed', model_name='gcn', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "run_experiments_similarity(seed=42, dataset_name='pubmed', model_name='sage', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C88-q5ZZwXa_"
      },
      "outputs": [],
      "source": [
        "# run_experiments_similarity(seed=11, dataset_name='amazonphoto', model_name='gat', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=10, dataset_name='amazonphoto', model_name='gatv2', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=123, dataset_name='amazonphoto', model_name='gcn', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=123, dataset_name='amazonphoto', model_name='sage', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um5IAPfjwXbA"
      },
      "outputs": [],
      "source": [
        "# run_experiments_similarity(seed=11, dataset_name='amazoncomputer', model_name='gat', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=10, dataset_name='amazonphoto', model_name='gatv2', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=123, dataset_name='amazoncomputer', model_name='gcn', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])\n",
        "# run_experiments_similarity(seed=30, dataset_name='amazoncomputer', model_name='sage', extend_metrics = [\"adamic_adar\", \"resource_alloc\", \"jaccard\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XjPqbZ159Fu"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download(results_file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5FneIBMOBxT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3MKenNAnFFHd",
        "LZk4YbP6SMLm",
        "GzhnNd60H7tl",
        "YoLS4SrUWdBY",
        "K4g8KklyhYa6",
        "H54z2YmJSVhw",
        "-2qB9kQyHlYr",
        "oIO5zaUk6rXb",
        "XymoOu7Vr4x4",
        "rNPvX_4jjfUK",
        "2nuXpJonjlIJ",
        "ligQibMc5v2Q"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
