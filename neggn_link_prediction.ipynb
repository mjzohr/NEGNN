{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YeGAzOLQXgP0"
      },
      "outputs": [],
      "source": [
        "!pip install dgl-cu111 dglgo -f https://data.dgl.ai/wheels/repo.html\n",
        "# !pip install dgl dglgo -f https://data.dgl.ai/wheels/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnxiTbx3bZ6Q",
        "outputId": "8889acf9-7f0a-485d-c2df-e1124944d849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n"
          ]
        }
      ],
      "source": [
        "import dgl.data\n",
        "dataset = dgl.data.CoraGraphDataset()\n",
        "# dataset = dgl.data.CiteseerGraphDataset()\n",
        "# dataset = dgl.data.PubmedGraphDataset()\n",
        "# dataset = dgl.data.AmazonCoBuyPhotoDataset()\n",
        "# dataset = dgl.data.AmazonCoBuyComputerDataset()\n",
        "g = dataset[0]\n",
        "\n",
        "# extend_metric: \"adamic_adar\", \"resource_alloc\", \"jaccard\", \"degree\", \"pagerank_degree\"\n",
        "# extend_metric = 'None'\n",
        "# extend_metric = 'degree'\n",
        "# extend_metric = 'adamic_adar'\n",
        "# extend_metric = 'resource_alloc'\n",
        "extend_metric = 'jaccard'\n",
        "# extend_metric = 'pagerank_degree'\n",
        "\n",
        "# model = 'sage'\n",
        "# model = 'gcn'\n",
        "model = 'gat'\n",
        "# model = 'gatv2'\n",
        "\n",
        "add_self_loop = True\n",
        "\n",
        "# alpha = int(3327 - 3327*0.2)\n",
        "alpha = int(2708 - 2708*0.2)\n",
        "beta = 50\n",
        "gamma_edges = 1500\n",
        "explore = 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yor12Mb9yHQS",
        "outputId": "6a9f40be-9343-4498-cb9f-85fa6f773a45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2166.4"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (3327 - 3327*0.2)\n",
        "2708- 2708*0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z5ZsAtPw2dvq"
      },
      "outputs": [],
      "source": [
        "# # extend_metric = 'degree'\n",
        "# extend_metric = 'pagerank_degree'\n",
        "\n",
        "# # model = 'sage'\n",
        "# model = 'gcn'\n",
        "# # model = 'gat'\n",
        "# # model = 'gatv2'\n",
        "# add_self_loop = True\n",
        "\n",
        "# alpha = 50000\n",
        "# beta = 100\n",
        "# # gamma_edges = 4000\n",
        "# # explore = 0.005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZKHaIIVnH7th"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import networkx as nx\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "from dgl.data import register_data_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hPXNYeCvQ40p"
      },
      "outputs": [],
      "source": [
        "seed=100\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "dgl.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZk4YbP6SMLm"
      },
      "source": [
        "### Neighborhood extention functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q-qFYOnHSIh8"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "import heapq\n",
        "import numpy as np\n",
        "import dgl\n",
        "from numpy.linalg import inv\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def centrality_based(centrality_metric, graph, graph_undirected, alpha, beta, dataset):\n",
        "\n",
        "    # these ones return a Dictionary of nodes with centrality as the value.\n",
        "    if centrality_metric == 'degree':\n",
        "        centrality = nx.degree_centrality(graph)\n",
        "    elif centrality_metric == 'eigenvector':\n",
        "        graph = nx.DiGraph(graph)\n",
        "        centrality = nx.eigenvector_centrality(graph)\n",
        "\n",
        "    important_nodes = heapq.nlargest(alpha, centrality, key=centrality.get)\n",
        "    \n",
        "    # TODO, should the selected nodes change for each imp_node?\n",
        "    new_dataset = dataset\n",
        "    for item in important_nodes:\n",
        "        tgt_nodes = set(graph_undirected) - set(graph_undirected[item])\n",
        "        selected_nodes = torch.tensor(np.random.choice(list(tgt_nodes), beta))\n",
        "        important_node = torch.ones(len(selected_nodes), dtype=int)*torch.tensor(item)\n",
        "        new_dataset = dgl.add_edges(new_dataset, selected_nodes, important_node)\n",
        "        new_dataset = dgl.add_edges(new_dataset, important_node, selected_nodes)    \n",
        "\n",
        "    return new_dataset\n",
        "\n",
        "\n",
        "def non_edges_important_nodes(graph, alpha, explore):\n",
        "    # nodes = set(graph)\n",
        "    # selected_src_nodes = np.random.choice(list(nodes), int(0.2*len(nodes)), replace=False)\n",
        "    # centrality = nx.eigenvector_centrality(graph)\n",
        "    centrality = nx.degree_centrality(graph)\n",
        "    selected_src_nodes = heapq.nlargest(alpha, centrality, key=centrality.get)\n",
        "\n",
        "    edges_list = []\n",
        "    for u in tqdm(selected_src_nodes):\n",
        "        tgt_nodes = set(graph) - set(graph[u])\n",
        "        selected_tgt_nodes = np.random.choice(list(tgt_nodes), int(explore*len(tgt_nodes)), replace=False)\n",
        "\n",
        "        edges = list(zip(np.ones(len(selected_tgt_nodes), dtype=int)*u, selected_tgt_nodes))\n",
        "        edges_list.extend(edges)\n",
        "\n",
        "    return edges_list\n",
        "\n",
        "\n",
        "def resource_allocation_index(G, dataset, gamma, alpha, explore, ebunch=None):\n",
        "    if ebunch is None:\n",
        "        ebunch = non_edges_important_nodes(G, alpha, explore)\n",
        "    \n",
        "    def predict(u, v):\n",
        "        return sum(1 / G.degree(w) for w in nx.common_neighbors(G, u, v))\n",
        "\n",
        "    edges = []\n",
        "    scores = []\n",
        "    for edge in tqdm(ebunch):\n",
        "        u, v = edge\n",
        "        try:\n",
        "          pred_val = predict(u, v)\n",
        "          if pred_val > 0:\n",
        "              edges.append((u,v))\n",
        "              scores.append(pred_val)\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "    scores = np.array(scores)\n",
        "    sum_scores = scores.sum()\n",
        "    scores /= sum_scores\n",
        "\n",
        "    rnd_indices = np.random.choice(len(edges), gamma, p=scores, replace=True)\n",
        "    selected_edges = [edges[i] for i in rnd_indices]\n",
        "\n",
        "    new_edges = torch.tensor(selected_edges)\n",
        "\n",
        "    new_edges_1 = new_edges[:,0]\n",
        "    new_edges_2 = new_edges[:,1]\n",
        "\n",
        "    new_dataset = dgl.add_edges(dataset, new_edges_1, new_edges_2)\n",
        "    new_dataset = dgl.add_edges(new_dataset, new_edges_2, new_edges_1)\n",
        "\n",
        "    return new_dataset\n",
        "\n",
        "\n",
        "def jaccard_coefficient_index(G, dataset, gamma, alpha, explore, ebunch=None):\n",
        "    if ebunch is None:\n",
        "        ebunch = non_edges_important_nodes(G, alpha, explore)\n",
        "    \n",
        "    def predict(u, v):\n",
        "        cnbors = list(nx.common_neighbors(G, u, v))\n",
        "        union_size = len(set(G[u]) | set(G[v]))\n",
        "        if union_size == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return len(cnbors) / union_size\n",
        "\n",
        "    edges = []\n",
        "    scores = []\n",
        "    for edge in tqdm(ebunch):\n",
        "        u, v = edge\n",
        "        try:\n",
        "          pred_val = predict(u, v)\n",
        "          if pred_val > 0:\n",
        "              edges.append((u,v))\n",
        "              scores.append(pred_val)\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "    scores = np.array(scores)\n",
        "    sum_scores = scores.sum()\n",
        "    scores /= sum_scores\n",
        "\n",
        "    rnd_indices = np.random.choice(len(edges), gamma, p=scores, replace=True)\n",
        "    selected_edges = [edges[i] for i in rnd_indices]\n",
        "\n",
        "    new_edges = torch.tensor(selected_edges)\n",
        "\n",
        "    new_edges_1 = new_edges[:,0]\n",
        "    new_edges_2 = new_edges[:,1]\n",
        "\n",
        "    new_dataset = dgl.add_edges(dataset, new_edges_1, new_edges_2)\n",
        "    new_dataset = dgl.add_edges(new_dataset, new_edges_2, new_edges_1)\n",
        "\n",
        "    return new_dataset\n",
        "\n",
        "\n",
        "def adamic_adar_index(G, dataset, gamma, alpha, explore, ebunch=None):\n",
        "    if ebunch is None:\n",
        "        ebunch = non_edges_important_nodes(G, alpha, explore)\n",
        "\n",
        "    def predict(u, v):\n",
        "        return sum(1 / math.log(G.degree(w))\n",
        "                   for w in nx.common_neighbors(G, u, v))\n",
        "\n",
        "    edges = []\n",
        "    scores = []\n",
        "    for edge in tqdm(ebunch):\n",
        "        u, v = edge\n",
        "        try:\n",
        "          pred_val = predict(u, v)\n",
        "          if pred_val > 0:\n",
        "              edges.append((u,v))\n",
        "              scores.append(pred_val)\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "    scores = np.array(scores)\n",
        "    sum_scores = scores.sum()\n",
        "    scores /= sum_scores\n",
        "\n",
        "    rnd_indices = np.random.choice(len(edges), gamma, p=scores, replace=True)\n",
        "    selected_edges = [edges[i] for i in rnd_indices]\n",
        "\n",
        "    new_edges = torch.tensor(selected_edges)\n",
        "\n",
        "    new_edges_1 = new_edges[:,0]\n",
        "    new_edges_2 = new_edges[:,1]\n",
        "\n",
        "    new_dataset = dgl.add_edges(dataset, new_edges_1, new_edges_2)\n",
        "    new_dataset = dgl.add_edges(new_dataset, new_edges_2, new_edges_1)\n",
        "\n",
        "    return new_dataset\n",
        "\n",
        "\n",
        "def pagerank_degree(graph, alpha, num_new_edges, dataset):\n",
        "    \n",
        "    centrality = nx.degree_centrality(graph)\n",
        "\n",
        "    important_nodes = heapq.nlargest(alpha, centrality, key=centrality.get)\n",
        "    \n",
        "    new_dataset = dataset\n",
        "    for node_id in tqdm(important_nodes):\n",
        "        ppr = nx.pagerank(graph, personalization={node_id:1})\n",
        "        important_tgt_nodes = torch.tensor(heapq.nlargest(num_new_edges+1, ppr, key=ppr.get))\n",
        "        important_tgt_nodes = important_tgt_nodes[1:]\n",
        "\n",
        "        node_source = torch.ones(num_new_edges, dtype=int)*torch.tensor(node_id)\n",
        "\n",
        "        new_dataset = dgl.add_edges(new_dataset, node_source, important_tgt_nodes)\n",
        "        new_dataset = dgl.add_edges(new_dataset, important_tgt_nodes, node_source)\n",
        "\n",
        "        return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzhnNd60H7tl"
      },
      "source": [
        "### Prepare training and testing sets\n",
        "\n",
        "10% for the test-set \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zU4X7Hn8R4pS"
      },
      "outputs": [],
      "source": [
        "def extend_neighborhood(dataset, graph_nx, graph_undirected, alpha, beta, gamma, explore, extend_metric):\n",
        "    if extend_metric == 'adamic_adar':\n",
        "        print(\"---Adamic Adar index---\")\n",
        "        extended_graph = adamic_adar_index(graph_undirected, dataset=dataset, gamma=gamma, alpha=alpha, explore=explore, ebunch=None)\n",
        "    elif extend_metric == 'resource_alloc':\n",
        "        print(\"---Resouce Allocation index---\")\n",
        "        extended_graph = resource_allocation_index(graph_undirected, dataset=dataset, gamma=gamma, alpha=alpha, explore=explore, ebunch=None)\n",
        "    elif extend_metric == 'jaccard':\n",
        "        print(\"---Jaccard Coefficient index---\")\n",
        "        extended_graph = jaccard_coefficient_index(graph_undirected, dataset=dataset, gamma=gamma, alpha=alpha, explore=explore, ebunch=None)\n",
        "    elif extend_metric == 'degree':\n",
        "        print(\"---Degree index---\")\n",
        "        extended_graph = centrality_based(centrality_metric='degree', graph=graph_nx, graph_undirected=graph_undirected, alpha=alpha, beta=beta, dataset=dataset)\n",
        "    elif extend_metric == 'pagerank_degree':\n",
        "        print(\"---PageRank-degree index---\")\n",
        "        extended_graph = pagerank_degree(graph=graph_nx, alpha=alpha, num_new_edges=beta, dataset=dataset)\n",
        "\n",
        "    return extended_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZFXT82eGH7tm"
      },
      "outputs": [],
      "source": [
        "# Split edge set for training and testing\n",
        "u, v = g.edges()\n",
        "\n",
        "eids = np.arange(g.number_of_edges())\n",
        "eids = np.random.permutation(eids)\n",
        "test_size = int(len(eids) * 0.2)\n",
        "train_size = g.number_of_edges() - test_size\n",
        "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
        "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
        "\n",
        "# Find all negative edges and split them for training and testing\n",
        "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
        "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
        "neg_u, neg_v = np.where(adj_neg != 0)\n",
        "\n",
        "neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
        "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
        "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UQxxTaSqH7to"
      },
      "outputs": [],
      "source": [
        "train_g = dgl.remove_edges(g, eids[:test_size])\n",
        "graph_nx = dgl.to_networkx(train_g)\n",
        "graph_undirected = nx.Graph(graph_nx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R-7A6L9LH7ts"
      },
      "outputs": [],
      "source": [
        "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
        "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
        "\n",
        "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
        "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoLS4SrUWdBY"
      },
      "source": [
        "### GNN models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Oj-NALRvzoU-"
      },
      "outputs": [],
      "source": [
        "from dgl.nn import SAGEConv\n",
        "from dgl.nn import GraphConv\n",
        "from dgl.nn import GATConv\n",
        "from dgl.nn import GATv2Conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_YwzxVKguS8m"
      },
      "outputs": [],
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 num_classes,\n",
        "                 heads,\n",
        "                 activation,\n",
        "                 feat_drop,\n",
        "                 attn_drop,\n",
        "                 negative_slope,\n",
        "                 residual):\n",
        "        super(GAT, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.gat_layers.append(GATConv(\n",
        "            in_dim, num_hidden, heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for l in range(1, num_layers):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.gat_layers.append(GATConv(\n",
        "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
        "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "        # output projection\n",
        "        self.gat_layers.append(GATConv(\n",
        "            num_hidden * heads[-2], num_classes, heads[-1],\n",
        "            feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.gat_layers[-1](g, h).mean(1)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "28QXsmO51baW"
      },
      "outputs": [],
      "source": [
        "class GATv2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 num_classes,\n",
        "                 heads,\n",
        "                 activation,\n",
        "                 feat_drop,\n",
        "                 attn_drop,\n",
        "                 negative_slope,\n",
        "                 residual):\n",
        "        super(GATv2, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.gat_layers.append(GATv2Conv(\n",
        "            in_dim, num_hidden, heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for l in range(1, num_layers):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.gat_layers.append(GATv2Conv(\n",
        "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
        "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "        # output projection\n",
        "        self.gat_layers.append(GATv2Conv(\n",
        "            num_hidden * heads[-2], num_classes, heads[-1],\n",
        "            feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.gat_layers[-1](g, h).mean(1)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7YyxBjA0WoM1"
      },
      "outputs": [],
      "source": [
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
        "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
        "    \n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, in_feats, h_feats):\n",
        "      super(GCN, self).__init__()\n",
        "      self.conv1 = GraphConv(in_feats, h_feats)\n",
        "      self.conv2 = GraphConv(h_feats, h_feats)\n",
        "  \n",
        "  def forward(self, g, in_feat):\n",
        "      h = self.conv1(g, in_feat)\n",
        "      h = F.relu(h)\n",
        "      h = self.conv2(g, h)\n",
        "      return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4IyLpmBbDXT"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHdURhJ8H7tq",
        "outputId": "59ca6ad4-6b52-4a60-b080-89e23d42d9e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GAT model is used\n"
          ]
        }
      ],
      "source": [
        "# build a two-layer GNN model\n",
        "if model == 'sage':\n",
        "    print(\"SAGE model is used\")\n",
        "    model = GraphSAGE(train_g.ndata['feat'].shape[1], 16)\n",
        "\n",
        "elif model == 'gcn':\n",
        "    print(\"GCN model is used\")\n",
        "    model = GCN(train_g.ndata['feat'].shape[1], 16)\n",
        "\n",
        "elif model == 'gat':\n",
        "    print(\"GAT model is used\")\n",
        "    num_heads, num_layers, num_out_heads = 8, 2, 1\n",
        "    num_hidden = n_classes = 16\n",
        "    in_drop, attn_drop, negative_slope, residual = 0.6, 0.6, 0.2, False\n",
        "\n",
        "    heads = ([num_heads] * num_layers) + [num_out_heads]\n",
        "    model = GAT(num_layers, train_g.ndata['feat'].shape[1], num_hidden, n_classes, heads, F.elu, in_drop, attn_drop, negative_slope, residual)\n",
        "\n",
        "elif model == 'gatv2':\n",
        "    print(\"GATv2 model is used\")\n",
        "    num_heads, num_layers, num_out_heads = 8, 2, 1\n",
        "    num_hidden = n_classes = 16\n",
        "    in_drop, attn_drop, negative_slope, residual = 0.6, 0.6, 0.2, False\n",
        "\n",
        "    heads = ([num_heads] * num_layers) + [num_out_heads]\n",
        "    model = GATv2(num_layers, train_g.ndata['feat'].shape[1], num_hidden, n_classes, heads, F.elu, in_drop, attn_drop, negative_slope, residual)\n",
        "\n",
        "else:\n",
        "    raise(\"Not a model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNxGJTNvUplb"
      },
      "source": [
        "max, sum, concat, avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QWDJsumtH7tt"
      },
      "outputs": [],
      "source": [
        "import dgl.function as fn\n",
        "\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            # Compute a new edge feature named 'score' by a dot-product between the\n",
        "            # source node feature 'h' and destination node feature 'h'.\n",
        "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
        "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
        "            return g.edata['score'][:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rpJmzPqJ7LqF"
      },
      "outputs": [],
      "source": [
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        # Computes a scalar score for each edge of the given graph.\n",
        "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
        "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NIjr-5eR7oDl"
      },
      "outputs": [],
      "source": [
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats, h_feats//2)\n",
        "        self.W2 = nn.Linear(h_feats//2, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        # Computes a scalar score for each edge of the given graph.\n",
        "        # Tested: max, min, cat, add, mean(add/2)\n",
        "        h = torch.mul(edges.src['h'], edges.dst['h'])\n",
        "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3DCkYv8gH7tv"
      },
      "outputs": [],
      "source": [
        "# pred = DotPredictor()\n",
        "pred = MLPPredictor(16)\n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "def compute_auc(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
        "    labels = torch.cat(\n",
        "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
        "    return roc_auc_score(labels, scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guHpOgvUdPRK",
        "outputId": "6540bd34-09ba-438f-e90b-05892875eb20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of edges before extension: 8445\n",
            "---Jaccard Coefficient index---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2166/2166 [00:02<00:00, 1020.89it/s]\n",
            " 55%|█████▍    | 1279163/2341663 [00:47<00:20, 52731.43it/s]"
          ]
        }
      ],
      "source": [
        "if extend_metric != 'None':\n",
        "    t1 = time.time()\n",
        "    num_edge_before_extention = train_g.number_of_edges()\n",
        "    print(f'Total number of edges before extension: {num_edge_before_extention}')\n",
        "    \n",
        "    train_g = extend_neighborhood(train_g, graph_nx, graph_undirected,  alpha, beta, gamma_edges, explore, extend_metric)\n",
        "    \n",
        "    t_total = time.time() - t1\n",
        "    num_edge_after_extention = train_g.number_of_edges()\n",
        "    print(f'Total number of edges after extension: {num_edge_after_extention}')\n",
        "    print(f'Total number of added edges: {num_edge_after_extention - num_edge_before_extention}')\n",
        "    print(f\"*** Total construction time in seconds: {t_total:.2f} ***\")\n",
        "\n",
        "else:\n",
        "    print(\"Neighborhood is not extended!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW6WJ1dKjXwe"
      },
      "outputs": [],
      "source": [
        "if add_self_loop == True:\n",
        "    print(f\"Total edges before adding self-loop {train_g.number_of_edges()}\")\n",
        "    train_g = train_g.remove_self_loop().add_self_loop()\n",
        "    print(f\"Total edges after adding self-loop {train_g.number_of_edges()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4g8KklyhYa6"
      },
      "source": [
        "### Train & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euvpyllTH7tw"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)\n",
        "\n",
        "all_logits = []\n",
        "for e in range(100):\n",
        "    # forward\n",
        "    h = model(train_g, train_g.ndata['feat'])\n",
        "    pos_score = pred(train_pos_g, h)\n",
        "    neg_score = pred(train_neg_g, h)\n",
        "    loss = compute_loss(pos_score, neg_score)\n",
        "    # backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if e % 5 == 0:\n",
        "        print('In epoch {}, loss: {}'.format(e, loss))\n",
        "\n",
        "# ----------- 5. check results ------------------------ #\n",
        "from sklearn.metrics import roc_auc_score\n",
        "with torch.no_grad():\n",
        "    pos_score = pred(test_pos_g, h)\n",
        "    neg_score = pred(test_neg_g, h)\n",
        "    print('AUC', compute_auc(pos_score, neg_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW5jqyErX0Zo"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85BbgrKJE3pc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LZk4YbP6SMLm",
        "YoLS4SrUWdBY"
      ],
      "name": "link_predict_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
